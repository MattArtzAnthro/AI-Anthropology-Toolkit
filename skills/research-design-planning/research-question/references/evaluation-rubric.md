# Evaluation Rubric and Stress-Testing

This reference provides tools for evaluating and stress-testing anthropological
research questions: a seven-criterion rubric, a scope-feasibility matrix, and
repair strategies for common weaknesses.

## Seven-Criterion Rubric

Use this rubric to evaluate any research question or question set. Each
criterion is scored on a 5-point scale. The purpose is to make quality
explicit as a multi-criterion judgment rather than a taste-based reaction.

When presenting evaluations to users, focus on the weakest criterion and
provide a specific repair suggestion. Do not dump all seven scores unless the
user asks for a full evaluation.

### 1. Theoretical Contribution

| Score | Description |
|-------|-------------|
| 5 | Poses a clear problem in a contemporary debate; implies "scaling up" from case to broader conceptual stakes |
| 4 | Strong engagement with literature; contribution is clear though not fully sharpened |
| 3 | Links to literature but contribution is partial or derivative |
| 2 | Mentions theory but does not specify how the project intervenes |
| 1 | Descriptive or "application only" without an intervention claim |

Repair strategy: Ask "What does this case force us to rethink?" If the answer
is "nothing — it confirms what we already know," the question lacks a
theoretical contribution. Look for tensions, surprises, or contradictions
between existing theory and what the ethnographic situation presents.

### 2. Empirical Tractability

| Score | Description |
|-------|-------------|
| 5 | Evidence types and methods are clearly implied and realistically obtainable within the stated constraints |
| 4 | Evidence is plausible; access plan has minor gaps |
| 3 | Evidence plausible but access/method plan is thin or assumed |
| 2 | Evidence types vaguely implied; feasibility unclear |
| 1 | Evidence not specified or unrealistic given the constraints |

Repair strategy: For each sub-question, name 2–3 specific data types that
would answer it. If you cannot, the question is not yet empirically anchored.
Also check: does the researcher actually have (or can they realistically
obtain) access to this evidence?

### 3. Coherence of Question Set

| Score | Description |
|-------|-------------|
| 5 | Governing question and sub-questions are non-redundant; each illuminates a different facet; together they constitute a complete response |
| 4 | Mostly coherent; one sub-question slightly overlaps or feels tangential |
| 3 | Some overlap or loose connection among sub-questions |
| 2 | Sub-questions drift across loosely related topics |
| 1 | Laundry list with no visible logic connecting the questions |

Repair strategy: Map each sub-question to a distinct evidence type or
analytical move. If two sub-questions would be answered by the same data using
the same method, they should be merged or one should be dropped. The governing
question should be recoverable from the sub-questions: if you answered all the
sub-questions, would you have answered the governing question?

### 4. Clarity and Rhetoric

| Score | Description |
|-------|-------------|
| 5 | Direct, non-performative phrasing; no vague time markers; readable to non-specialists |
| 4 | Mostly clear; minor jargon or one vague marker |
| 3 | Some jargon, temporal vagueness, or self-referential framing |
| 2 | Heavily metadiscursive or reliant on insider vocabulary |
| 1 | Unclear to anyone outside the immediate subfield |

Repair strategy: Read the question aloud. Can a smart colleague in a different
subfield understand what you are asking and why it matters? Remove "I argue,"
"this study explores," and similar self-referential frames. Replace "recently"
with a date range. Replace jargon with the concept it actually points to, then
decide if the jargon term is necessary.

### 5. Feasibility and Contingency

| Score | Description |
|-------|-------------|
| 5 | Includes explicit contingency logic; question survives "Plan B" scenario |
| 4 | Feasibility is well-argued; contingency is implicit but recoverable |
| 3 | Feasibility assumed rather than argued; no contingency |
| 2 | Significant access or resource concerns not addressed |
| 1 | No contingency; fragile to predictable disruption |

Repair strategy: Apply the "Plan B" test from Wenner-Gren guidelines — if
your primary field site becomes inaccessible, if a key population withdraws,
or if conditions change dramatically, can you still address the research
question via an alternative empirical strategy? If not, the question is
overfit to one site or population. The theoretical problem should be separable
from any single empirical instantiation.

### 6. Ethics and Responsibility

| Score | Description |
|-------|-------------|
| 5 | Obligations, harms, benefits, and governance are built into the question's framing and are revisitable |
| 4 | Ethics are visible in the question design; minor gaps in governance or benefit language |
| 3 | Ethics treated as IRB compliance only; not integrated into the question's intellectual structure |
| 2 | Ethics mentioned but perfunctory |
| 1 | Ethics absent from the question's design |

Repair strategy: Check whether the question implies extraction without
reciprocity. Can you articulate: Who benefits from this knowledge? What
obligations does the research create? How will consent be maintained (not
just obtained) over time? What harms could arise, including downstream harms
from how the knowledge is used? If these are absent, the question needs an
ethical reframe, not just an IRB protocol.

For research involving Indigenous communities, descendant communities, human
remains, or sensitive data: apply the CARE Principles (Collective benefit,
Authority to control, Responsibility, Ethics) as a supplementary check. The
question should specify not only "what will be known" but "for whom," "under
what governance," and "with what downstream constraints."

### 7. Fit to Document Genre

| Score | Description |
|-------|-------------|
| 5 | Calibrated to the target genre's word limits, structure, and rhetorical expectations |
| 4 | Mostly fits; minor adjustments needed for the specific venue |
| 3 | Some mismatch between question scope and genre constraints |
| 2 | Question scope exceeds what the genre can responsibly deliver |
| 1 | Unanswerable within the document's length or structure |

Repair strategy: Check the target venue's word limits. A single journal
article (7,500–11,000 words in most generalist anthro journals) can
responsibly answer one governing question with 2–3 sub-questions. A
dissertation can handle a larger question set but each chapter needs its own
answerable sub-question. A grant application needs questions that map to a
timeline and budget. If the question requires mapping an entire national
policy field, multi-generational histories, and three comparison sites within
a single article, it needs to be split or scoped down.

## Scope-Feasibility Matrix

Use this matrix to stress-test each dimension of a draft question. The
"productive middle" column is the target.

| Dimension | "Too open" signal | "Over-locked" signal | Productive middle |
|-----------|-------------------|----------------------|-------------------|
| Population / unit | "People today" | A single named individual with no comparative logic | A defined group with justified boundaries and a realistic access route |
| Time | "Recently," "in the modern era" | "The first week of March" (unless historically necessary) | A bounded period that matches data collection capacity and lets you trace change |
| Mechanism | Vague verbs: "affects," "impacts" | Mechanism assumed rather than investigated | Process verbs that specify how: negotiate, translate, classify, maintain |
| Evidence | No implied evidence form | Evidence pre-answers the question | Evidence implied but plural: observation + texts, bones + protocols, models + context |
| Ethics | Ethics externalized to IRB | Claims of "harm-free" research | Obligations specified and revisitable: consent, governance, benefits, debts |

## The "Plan B" Test

Major funders (notably Wenner-Gren) explicitly ask applicants to develop a
contingency plan that still addresses the research questions if conditions
change dramatically. This is not just a grant-writing exercise — it is a
design principle for robust questions.

Apply this test:

1. Name the most likely disruption to your fieldwork or data access
   (political instability, institutional access revoked, pandemic restrictions,
   community withdrawal of consent, platform terms-of-service change for
   digital ethnography).
2. Ask: can the governing question still be pursued through an alternative
   empirical strategy?
3. If yes, the question is robust. If no, the question is overfit to one
   access pathway and should be reframed at a slightly higher level of
   abstraction while keeping empirical specificity in the sub-questions.

## Quick Diagnostic Checklist

Use this as a fast pre-flight check before presenting a question set. Each
check should be answerable in one sentence.

| Check | One-sentence answer required |
|-------|------------------------------|
| Theoretical lever | "The question clarifies a tension in [specific debate]." |
| Empirical anchor | "I can observe/measure/trace [specific evidence] in [specific setting]." |
| Answer-form | "An answer will look like [specific analytical product], not 'everything about the topic.'" |
| Ethics as design | "Consent/governance/benefit is built into the question's assumptions." |
| Contingency | "If access changes, I can still address the question via [specific alternative]." |
| Genre fit | "The question can be expressed in a title + abstract for [specific venue]." |

If any check produces a vague or evasive answer, that dimension needs work
before the question is ready.

## Repair Moves for Common Problems

### "My advisor says it's too broad"

Usually means one of:
- Missing conceptual lever (it's a topic, not a question)
- Missing situated context (it could be studied anywhere, which means it's
  not specific enough to be studied well anywhere)
- Too many sub-questions (scope inflation)

Fix: Identify the single most interesting tension in the topic and build the
governing question around it. Kill sub-questions that don't directly serve the
governing question.

### "My advisor says it's too narrow"

Usually means one of:
- Missing "scaling up" — the question answers itself at the case level but
  doesn't connect to broader debates
- The conceptual lever is implicit (the researcher knows why it matters but
  hasn't said so)

Fix: Add the "and what does this reveal about [broader concept]?" clause.
Make the theoretical stakes explicit.

### "I have questions but they feel disconnected"

Usually means the governing question is implicit or absent. The sub-questions
are each interesting but aren't visibly connected to a single intellectual
problem.

Fix: Write the governing question last — look at the sub-questions and ask
"what is the one question that, if answered, would tie all of these together?"
Then revise the sub-questions to serve it.

### "I keep rewriting and it never feels right"

Often a sign of theoretical indecision rather than a wording problem. The
researcher may be torn between two frameworks or two scales of analysis.

Fix: Name the tension explicitly. Sometimes the right governing question is
about the tension itself. Or: draft two separate question sets for each
framework and see which one generates more analytical traction.
